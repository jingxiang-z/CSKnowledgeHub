# 基础概念

[toc]

## ML基础概念类

### 基础

1.overfitting/underfiting是指的什么

过拟合：模型过于复杂，泛化误差高，在训练数据集上表现好，在测试数据集上表现差

欠拟合：模型过于简单，训练误差高，在训练数据集和测试数据集上表现差

2.bias/variance trade off 是指的什么

**偏差**度量了学习算法的期望预测与真实结果的偏离程度，刻画了学习算法本身的拟合能力

**方差**度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响

学习器的泛化误差可以分解为偏差+方差+噪声

![img](../../../Images/HRwP1uQhVj9zEqo.jpg)

偏差小的模型，方差大；方差小的模型，偏差大；存在一个偏差与方差都较低的点，此时模型泛化误差最小

3.欠拟合，过拟合一般有哪些预防手段

**解决欠拟合的方法：**

```text
1、增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间
2、尝试非线性模型，比如核SVM 、决策树、DNN等模型
3、如果有正则项可以较小正则项参数 lambda
4、Boosting ,Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等
```

**解决过拟合的方法：**

```text
1、交叉检验，通过交叉检验得到较优的模型参数
2、特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间
3、正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择
4、如果有正则项则可以考虑增大正则项参数 lambda
5、增加训练数据可以有限的避免过拟合
6、Bagging ,将多个弱学习器Bagging 一下效果会好很多，比如随机森林等
```

4.Generative和Discrimitive的区别

Generative: 求联合概率分布$P(X, Y)$，利用贝叶斯公式求条件概率分布$P(Y|X)=\frac{P(X,Y)}{P(X)}$

Discrimitive: 直接求条件概率分布$P(Y|X)$

![img](../../../Images/Sbs79dz2qLZrth8.jpg)

5.Give a set of ground truths and 2 models, how do you be confident that one model is better than another?

使用模型预测的准确率、精确率、召回率、平方误差率等指标进行评判

期望风险：模型在预测数据集上的表现

经验风险：模型在训练数据集上的表现

结构风险：模型的复杂程度

期望风险难求，用经验风险和结构风险之和近似

### Reguarlization

1.L1 vs L2, which one is which and difference

正则化项是模型的结构风险，通过最小化结构风险避免模型发生严重的过拟合，目标函数为原始目标函数加正则化项，正则化项如下
$$
\lambda \sum_{i=1}^{M}|w_j|^q
$$
q=1，L1正则化，可以得到稀疏解，通过近端梯度下降等方法求解

q=2，L2正则化，求导梯度下降求解最优值，无法得到稀疏解

如下图所示，求解目标函数最小值，原始目标函数为蓝圈，正则化项为红圈，相交点处目标函数最小

![img](../../../Images/CL84sPApiofEzGX.jpg)

2.Lasso/Ridge的解释 (prior分别是什么）

Lasso: 线性回归+L1正则化项，等价于给线性回归加拉普拉斯先验
$$
J=\frac{1}{n}\sum_{i=1}^n(f(x_i)-y_i)^2+\lambda||w||_1
$$
Ridge: 线性回归+L2正则化项，等价于给线性回归加高斯先验
$$
J=\frac{1}{n}\sum_{i=1}^n(f(x_i)-y_i)^2+\lambda||w||^2_2
$$
最大后验估计：
$$
P(\theta|y)=\frac{P(y|\theta)P(\theta)}{P(y)}
$$

$$
\theta=arg max P(\theta|y)=argmaxP(y|\theta)P(\theta)
$$

$P(\theta)$为事先引入的先验知识，$P(y|\theta)$服从正态分布

3.为什么regularization works

限制模型的复杂度，使得部分参数较小或者为零，减小模型的结构风险

4.为什么regularization用L1 L2，而不是L3, L4..

L1正则化可以得到稀疏解，L2正则化有唯一解，L3，L4正则化倒数为高次多项式，存在多个零点，可能有多个最小值

### Metric

1.分类问题评判标准

TP：True Positive ：做出Positive的判定，而且判定是正确的
FP：False Positive ：做出Positive的判定，而且判定是错误的
TN：True Negative ：正确的Negative判定
FN：False Negative：错误的Negative判定

准确率Accuracy：TP+TN/TP+FP+TN+FN

精确率Precision：TP/TP+FP

召回率Recall：TP/TP+FN

F1得分：1/F1 = 1/P + 1/R

ROC曲线与AUC值

ROC曲线按照排序的顺序逐一按照正例预测，以“真正例率”（True Positive Rate，简称TPR）为纵轴，横轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏，AUC值即为ROC曲线下的面积，当AUC>0.5时，说明分类器有效，AUC=0.5时相当于随机猜测，AUC<0.5时，负向预测有效
$$
TPR=\frac{TP}{TP+FN}
$$

$$
FPR=\frac{FP}{TN+FP}
$$

![13.png](../../../Images/5bc71ed75cefe.png)

如下图所示，真正例与真负例服从分布，若两个分布无重叠，AUC=1，有重叠时AUC<1

![Image for post](../../../Images/IckhsWpVL8TDnYi.png)

2.label 不平衡时用什么metric

精确率、召回率、F1得分：精确率与召回率考虑了模型预测的查准、查全

AUC-ROC曲线：AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价

3.回归问题评判标准

MSE：平均平方误差
$$
MSE=\frac{1}{n}\sum_{i=1}^n||y_i-\hat y_i||_2^2
$$
MAE：平均绝对误差
$$
MAE=\frac{1}{n}\sum_{i=1}^n|y_i-\hat y_i|
$$
可决系数R方：反映自变量x对因变量y的可解释程度



4.confusion matrix

![5.png](../../../Images/5bc71daf871a6.png)

 

### Loss与优化

1.用MSE做loss的Logistic Rregression是convex problem吗

非凸优化问题

2.解释并写出MSE的公式, 什么时候用到MSE?
$$
MSE=\frac{1}{n}\sum_{i=1}^n(x_i-\hat x_i)^2
$$
MSE用于评判回归模型的效果

3.Linear Regression最小二乘法和MLE关系

最小二乘法通过最小化平均平方误差来得到参数值，MLE通过最大化似然函数值来得到参数值，这两种最终求得的LOSS function是相同的

4.什么是relative entropy/cross entropy, 以及K-L divergence 他们intuition

相对熵即K-L散度，衡量两个事件/分布的不同
$$
D_{KL}(A||B)=\sum_i-P_A(x_i)log(P_B(x_i))-\sum_i-P_A(x_i)log(P_A(x_i))=\sum_i P_A(x_i)log(\frac{P_A(x_i)}{P_B(x_i)})
$$

交叉熵为相对熵公式前半部分
$$
\sum_i-P_A(x_i)log(P_B(x_i))
$$
在机器学习问题中，$P_A(x_i)$为真实分布，$P_B(x_i)$为近似分布，相对熵后半部分为常数，相对熵达到最小值的时候，也意味着交叉熵达到了最小值。对$P_B(x_i)$的优化就等效于求交叉熵的最小值。另外，对交叉熵求最小值，也等效于求最大似然估计

## Linear Regression线性回归

1.Linear Regression的基础假设是什么

线性性：因变量和自变量满足线性关系

独立性（误差项）：误差项之间相互独立

独立性（自变量）：各个自变量之间相互独立

正态性：误差项应呈正态分布

同方差性：误差项的方差为常数

2.what will happen when we have correlated variables, how to solve

删除变量：这个方法一般不推荐使用，因为删除变量会导致异方差增大

增加样本容量：增大样本量，减小多重共线性

变换模型：对数据求差分；计算相对指标；相关变量做线性组合，例如使用主成分分析等方法降维

逐步回归：常用方法，添加删除变量之后做可决系数、F检验和T检验来确定是否增加或者剔除变量，若果增加变量对这些指标的影响较小，也认为指标为多余的，如果增加指标引起R和F的变动且通不过T检验，说明存在共线性

岭回归：Ridge regression加入正则化项，减小自变量系数，减弱多重共线性

3.explain regression coefficient

回归分析只能得出相关关系，不能判断因果关系

当其他变量保持不变或控制其他变量不变时$X$每改变一个单位时因变量$Y$的平均变化量

4.what is the relationship between minimizing squared error and maximizing the likelihood

两种不同的求最优解的方法，最终计算出的结果是一致的

最小二乘法(OLS)基于频率派概率思想，求真值与估计值之间差值最小对应的参数

最大似然(MLE)基于贝叶斯思想，将参数看作概率分布，通过最大化似然函数来求参数

5.if the relationship between y and x is no linear, can linear regression solve that

可以添加高次变量，以及不同变量之间的组合，例如$x_1^2$ $x_1x_2$等交互变量作为自变量

## Logistic回归

1.Logistic Regression的loss是什么

logistic regression的损失函数为交叉熵损失函数
$$
J(\theta)=-[ylog(h(x))+(1-y)log(1-h(x))]
$$

## KNN

1.KNN算法

输入训练数据集，输出实例x所属的类y

(1)根据给定的距离算法，在训练集找出与x最邻近的k个点

(2)根据这些点多数点所属类别确定这个点的类别

KNN没有显式的学习过程，没有模型参数

2.KNN距离
$$
L_p(x_i,x_j)=(\sum|x_i-x_j|^p)^{1/p}
$$
P=1，称为曼哈顿距离

P=2，称为欧式距离

P=无穷时，是每个坐标距离的最大值

不同距离度量确定的最近邻点是不同的

 3.k值选择

k值较小，训练误差小，易发生过拟合。K=1时训练误差为零

k值较大，训练误差大，模型变简单，易发生欠拟合。K=N时类似于少数服从多数

在应用过程中，通过交叉验证(cross-validation)取验证误差最小时对应的k

4.KNN的实现：kd树

kd树是一种二叉树，表示对k维空间的一个划分

kd树的构造：

(1)构造根节点，使其对应于包含所有数据的k维空间超矩形区域，选择x1为坐标轴，以所有数据点x1值的中位数作为切分点，将超矩形区域分为两个子区域并生成根节点的左右子节点，分别对应小于和大于x1坐标的子区域，落在切分平面上的点保留在根节点

(2)对深度为j的节点，重复上述过程并生成j+1层的子节点直到两个子区域没有实例存在时停止

kd树的搜索：

(1)找到包含目标点x的叶节点（从根节点向下遍历）

(2)以此叶节点为当前最近点

(3)递归向上回退，先检查父节点是否更近，若是则更新最近点，若不是则检查另一子节点对应区域是否相交，若相交则在另一子节点对应区域查找，若不相交则回退至父节点

(4)按步骤3递归进行回退直到根节点

kd树搜索适用于训练实例树大于空间维度时的搜索，平均计算复杂度时O(logN)

## 决策树

1.概念

从逻辑角度，决策树是一些if else语句的组合；从几何角度，是根据某种准则划分特征空间；最终的目的是样本分类纯度高

决策树模型学习步骤：特征选择、决策树生成、决策树修剪

性质：决策树的路径互斥并且完备，每一个实例都被一条路径或者一条规则所覆盖，且仅被一条路径或规则所覆盖

学习目标：根据给定的训练数据构建模型，使其能够对实例进行正确分类，得到一个与训练数据矛盾较小，且具有较强泛化能力的模型 

2.特征选择：

选择准则：信息增益或信息增益比

随机变量X的熵：$H(X)=-\sum p_i \log p_i$

条件熵：$H(Y|X)=\sum p_iH(Y|X=x_i)$

信息增益：$g(D,A)=H(D)-H(D|A)$ 信息增益表示由于特征A而使得对数据集D的分类的不确定性减少程度

信息增益比：$g_R(D,A)=g(D,A)/H(D)$

3.决策树的生成

ID3算法：从根节点开始，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，由该特征的不同取值建立子节点；再对子节点递归调用上述方法，构建决策树，直到所有节点信息增益小于某个阈值为止

C4.5算法：与ID3算法类似，选择信息增益比代替信息增益作为选择标准 

4.决策树的剪枝

在决策树的学习中将已生成的树进行简化的过程称为剪枝

决策树的损失函数如下，T为叶节点个数，N为第t个叶节点样本数，加L1正则化项
$$
C(T)=\sum N_tH_t(T)+\alpha |T|
$$
在具体操作过程中，考虑剪枝前后树的熵，若剪枝后熵小于剪枝前熵，则进行剪枝，这种算法可以用动态规划的方法实现

5.CART算法

分类树与回归树算法，CART假设决策树是二叉树，内部节点特征的取值为是和否，左分支为取值为是的分支，右分支为取值为否的分支，使用基尼系数来选择最优变量的最优拆分点

回归树：回归树的训练过程采用启发式的方法选择第i个变量和他的切分点s，通过最小化误差函数的方法选择最优切分变量和切分点，每个节点样本的均值作为测试样本的回归预测值

(1)选择最优切分变量与切分点
$$
min_{j,s}[min \sum_{R_1(j,s)}(y_i-c_1)^2+min\sum_{R_2(j,s)}(y_i-c_2)^2]
$$
遍历j，对固定的切分变量j，扫描切分点，使得上式达到最小，得到(j,s)

(2)用选定的(j,s)划分区域并决定输出值

(3)继续对子区域调用步骤1、2

(4)输入空间划分为M个区域，生成决策树

分类树：计算现有特征对数据集的基尼指数，在所有特征和切分点中选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点，每个节点样本的类别情况投票决定测试样本的类别

基尼指数：$Gini(p)=\sum p_k(1-p_k)$

基尼指数值越大，样本集合不确定性越大，与熵类似

(1)训练数据集为D，计算现有特征所有可能取值对数据集的基尼指数

(2)在所有可能特征和它们的切分点a中选择基尼指数最小的特征及其切分点作为最优特征与最优切分点，并生成两个子节点

(3)对子节点递归调用1、2

(4)生成决策树

CART剪枝：从生成算法产生的决策树底端开始不断剪枝，直到根节点，形成子树序列，然后通过交叉验证集对子树序列进行测试，选择最优子树

6.决策树的剪枝

可以通过预剪枝或者后剪枝的方法预防过拟合

预剪枝策略：

定义一个高度，当决策树达到该高度时就可以停止决策树的生长

达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长

定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长

定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长

后剪枝策略：删除一些子树，然后用其叶子节点代替

REP方法是一种比较简单的后剪枝的方法，在该方法中，可用的数据被分成两个样例集合：一个训练集用来形成学习到的决策树，一个分离的验证集用来评估这个决策树在后续数据上的精度，确切地说是用来评估修剪这个决策树的影响

PEP,悲观错误剪枝,悲观错误剪枝法是根据剪枝前后的错误率来判定子树的修剪

7.决策树的正则化

通过设置最大深度，节点分裂阈值等方法来完成正则化

## 朴素贝叶斯

1.贝叶斯公式
$$
P(A|B)=\frac{P(A,B)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}
$$
学习目标：联合概率分布$P(A,B)$

加入先验知识$P(A)$

最大似然估计与最大后验估计

MLE: 最大化似然函数

MAP: 最大化后验概率，估计得到参数的后验分布

伯努利分布：MAP先验分布选择beta分布，后验与先验分布形式一致

多项式分布：MAP先验分布选择dirichlet分布 

3.朴素贝叶斯分类器
$$
P(Y|X_1,X_2,...,X_N)=\frac{P(X_1,X_2,...,X_N|Y)P(Y)}{P(X_1,X_2,...,X_N)}
$$
左侧需要参数过多，为$2^{n+1}-1$个

朴素贝叶斯：假设feature是独立的

目标函数：$arg max P(Y=y_k)\pi P(X_i|Y=y_k)$

4.朴素贝叶斯与逻辑回归

朴素贝叶斯相比于逻辑回归收敛更快，需要样本更少

数据量足够大时，逻辑回归效果更好，考虑了不同变量之间的相关关系

## 支持向量机SVM

1.线性可分支持向量机

支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。

分离超平面：$wx+b=0$

分类决策函数：$f(x)=sign(wx+b)$

函数间隔：$y_i(wx_i+b)$

几何间隔：$\frac{y_i}{||w||}(wx_i+b)$

将间隔最大化问题转化为最优化问题：
$$
min \frac{1}{2}||w||^2
$$

$$
s.t. y_i(wx_i+b)-1\ge 0, i=1,2,...,N
$$

间隔最大化算法求出的分离超平面解是唯一的

支持向量：训练集中离分离超平面距离最近的样本点

间隔：正负支持向量与分离超平面之间距离之和，为$2/||w||$

对偶问题：构造拉格朗日函数求解
$$
L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N \alpha_iy_i(wx_i+b)+\sum_{i=1}^N \alpha_i
$$
2.线性不可分支持向量机

相比于线性可分支持向量机，引入松弛变量$\epsilon_i$，允许存在错误分类的点。通过构造拉格朗日函数求解约束优化问题
$$
min \frac{1}{2}||w||^2
$$

$$
s.t. y_i(wx_i+b)-1\ge 1-\epsilon_i, i=1,2,...,N
$$

3.非线性支持向量机

对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机

在非线性支持向量机学习的对偶问题里，目标函数和分类决策函数都只涉及实例和实例之间的内积，所以不需要显式地指定非线性变换**，**而是用核函数替换当中的内积。

$$
K(x,z)=\phi (x)\sdot\phi(z)
$$
![img](../../../Images/oIqyPETceM7xbnD.jpg)

4.怎么把SVM的output按照概率输出

SVM分类器能输出（测试）样本和决策边界的距离，可以把这个距离当做一个置信度数值。可以用Logistic Regression把SVM输出的置信度数值校准为概率值

## Ensemble Learning集成学习

1.集成学习分类

一般来说集成学习可以分为三大类：

- 用于减少方差的bagging
- 用于减少偏差的boosting
- 用于提升预测结果的stacking

集成学习方法也可以归为如下两大类：

- 串行集成方法，这种方法串行地生成基础模型（如AdaBoost）。串行集成的基本动机是利用基础模型之间的依赖。通过给错分样本一个较大的权重来提升性能。
- 并行集成方法，这种方法并行地生成基础模型（如Random Forest）。并行集成的基本动机是利用基础模型的独立性，因为通过平均能够较大地降低误差。

2.Bagging

Bagging使用装袋采样来获取数据子集训练基础学习器。通常分类任务使用投票的方式集成，而回归任务通过平均的方式集成。Bagging中基学习器的数据是从所有数据中随机选择的，用于学习的特征也是随机选择一部分

在随机森林中，每个树模型都是装袋采样训练的。另外，特征也是随机选择的，最后对于训练好的树也是随机选择的，这种处理的结果是随机森林的偏差增加的很少，而由于弱相关树模型的平均，方差也得以降低，最终得到一个方差小，偏差也小的模型。

3.Boosting

Boosting指的是通过算法集合将弱学习器转换为强学习器。boosting的主要原则是训练一系列的弱学习器，所谓弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是**利用加权的数据**。在训练的早期对于错分数据给予较大的权重。 

AdaBoost第一个分类器y1(x)是用相等的权重系数进行训练的。在随后的boosting中，错分的数据权重系数将会增加，正确分类的数据权重系数将会减小。

梯度树提升（Gradient Tree Boosting）是一个boosting算法在损失函数上的泛化。能够用于分类和回归问题。Gradient Boosting采用串行方式构建模型。

4.Stacking

Stacking是通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术。基础模型利用整个训练集做训练，元模型将基础模型的特征作为特征进行训练。

基础模型通常包含不同的学习算法，因此stacking通常是异质集成。

4.difference between bagging and boosting

样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整

样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大

预测函数：

Bagging：所有预测函数的权重相等

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重

并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

## 树模型Boosting

### GBDT

GBDT是通过Boosting的思路来对决策树模型进行提升。GBDT采用加法模型，每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练。 gbdt对弱分类器的要求一般是足够简单， 并且低方差高偏差，因此每棵分类回归树的深度较浅，最终的总分类器在每轮训练的弱分类器加权求和得到。

GBDT使用的弱分类器是CART回归树，但是可以用于分类问题也可以用于回归问题，分类与回归所对应的损失函数不同

• 分类问题：$Loss = \sum_i -y_i\log(p_i)-(1-y_i)\log(1-p_i)$

• 回归问题：$Loss = \sum_i (y_i - \hat y_i)^2$

### XGBoost

- 首先，对所有特征都按照特征的数值进行预排序。
- 其次，在遍历分割点的时候用O(data)的代价找到一个特征上的最好分割点。
- 最后，找到一个特征的分割点后，将数据分裂成左右子节点。
- 优点：

算法本身的优化：在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以直接很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而**XGBoost损失函数对误差部分做二阶泰勒展开**，更加准确。算法本身的优化是我们后面讨论的重点。

算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。

算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。

- 缺点：

首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。

其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。

最后，对 cache 优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对 cache 进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的 cache miss。  

### LightGBM

**直方图算法**

基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

优点：

内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用 8 位整型存储就足够了，内存消耗可以降低为原来的1/8

计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data*#feature)优化到O(k*#features)

**带深度限制的 Leaf-wise 的叶子生长策略**

Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。  

**直方图加速**

一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍

**LightGBM并行优化**

LightGBM 还具有支持高效并行的优点。LightGBM 原生支持并行学习，目前支持**特征并行**和**数据并行**的两种。

- 特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。
- 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。

在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信；

在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。

## 树模型Bagging

### Random Forest

随机森林是采用Bagging的思想，构建多棵决策树，再通过多棵决策树模型平均的方法构建学习器。对于不同树的样本，通过bootstrap采样的方法得到，不同树采用的特征，也通过bootstrap采样得到

随机森林随机选择数据集和特征，特征数量小于总特征，多个分类器的平均结果就是最终预测的结果，而多个分类器的方差相似，去平均后方差为原来的1/n，所以随机森林可以减小方差，但是均值与单个模型的均值相同，所以不能减小偏差

GBDT和Random Forest区别：

• 随机森林采用的bagging思想，而GBDT采用的boosting思想

• 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成

• 组成随机森林的树可以并行生成；而GBDT只能是串行生成

• 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来

• 随机森林对异常值不敏感；GBDT对异常值非常敏感

• 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成

• 随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能

## 降维

1.优势

高维数据可视化

节省时间、内存

更好的泛化能力

去除噪音

2.矩阵分解

B为新空间的基，C为A在新空间的坐标

3.主成分分析

N个样本，P个维度，组成数据矩阵

每个维度进行去中心化

4.LDA

5.NMF

1.Explain PCA

主成分分析是一种统计分析、简化数据集的方法。它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分

2.LDA是什么，假设是什么

LDA算法的思想是将数据投影到低维空间之后，使得同一类数据尽可能的紧凑，不同类的数据尽可能分散。与PCA不同的是，LDA是一种监督学习方法

## 聚类

### 距离度量

有序变量：欧式距离、曼哈顿距离

无序变量：VDM距离

### 原型聚类

1.k均值聚类：最小化平方误差，通过迭代的方法更新簇中心

优化目标是最小化点与类别中心点之间距离的平方和

首先选取类别个数k并初始化类别中心，计算点到类别中心的距离并将其归入距离最小的类别，重新计算类别中心并反复迭代；当某次迭代与上一次迭代后所有点的类别都没有发生改变，则聚类停止

k均值聚类最终的类别划分与初始聚类中心的选择有关，收敛到局部最小值，可以尝试选择不同初始值进行聚类

K-means与KNN的区别：K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。

2.学习向量量化：初始化一组原型向量，更新

3.高斯混合聚类：样本的生成过程由高斯混合分布给出，通过EM算法求解

高斯混合模型能提供更强的描述能力，因为聚类时数据点的从属关系不仅与近邻相关，还会依赖于类簇的形状。n维高斯分布的形状由每个类簇的协方差来决定。在协方差矩阵上添加特定的约束条件后，可能会通过GMM和k-means得到相同的结果。

### 密度聚类

DBSCAN：一簇样本点为有密度可达关系导出的最大密度相连样本集合

$\epsilon$邻域：对于每个样本，它周围距离 $\epsilon$以内的样本的集合

核心对象：$\epsilon$邻域中样本数目大于MinPts的样本

密度直达：核心对象$\epsilon$邻域内的所有样本由这个核心对象密度直达

密度可达：核心对象密度直达具有传递性，由这种传递性连接的两个核心对象称为密度可达

密度相连：核心对象$\epsilon$邻域里的两个点密度相连

聚类过程：任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止

### 层次聚类

将数据集的每个样本堪称初始聚类簇，找出距离最近的两个聚类簇进行合并，重复该过程直到达到预设的聚类簇个数

## EM算法

对于含有隐变量的模型，极大似然法不方便求解，通过Jensen不等式进行放缩，极大化似然函数的下界，E步求期望，M步求期望的极大值来得到新的参数，这样反复迭代后，求出最优解